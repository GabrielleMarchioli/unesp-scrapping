{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "init_libs",
            "metadata": {},
            "outputs": [],
            "source": [
                "import requests\n",
                "from bs4 import BeautifulSoup\n",
                "import pandas as pd\n",
                "import sqlite3\n",
                "import time\n",
                "import re\n",
                "import plotly.express as px\n",
                "import plotly.io as pio\n",
                "\n",
                "pio.renderers.default = \"notebook_connected\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "config",
            "metadata": {},
            "outputs": [],
            "source": [
                "BASE_URL_TEMPLATE = \"https://dataprivacy.com.br/category/noticias/page/{}/\"\n",
                "DATABASE_NAME = \"dataprivacy.db\"\n",
                "HEADERS = {\n",
                "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
                "}\n",
                "\n",
                "print(\"Configura√ß√£o pronta!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "db_setup",
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_database():\n",
                "    conn = sqlite3.connect(DATABASE_NAME)\n",
                "    cursor = conn.cursor()\n",
                "    cursor.execute(\"\"\"\n",
                "        CREATE TABLE IF NOT EXISTS articles (\n",
                "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
                "            title TEXT,\n",
                "            date TEXT,\n",
                "            url TEXT UNIQUE,\n",
                "            source TEXT\n",
                "        )\n",
                "    \"\"\")\n",
                "    conn.commit()\n",
                "    conn.close()\n",
                "    print(\"‚úÖ Banco e tabela prontos\")\n",
                "\n",
                "create_database()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "insert_func",
            "metadata": {},
            "outputs": [],
            "source": [
                "def insert_article(title, date, url, source=\"Data Privacy Brasil\"):\n",
                "    conn = sqlite3.connect(DATABASE_NAME)\n",
                "    cursor = conn.cursor()\n",
                "    try:\n",
                "        cursor.execute(\"\"\"\n",
                "            INSERT INTO articles (title, date, url, source)\n",
                "            VALUES (?, ?, ?, ?)\n",
                "        \"\"\", (title, date, url, source))\n",
                "        conn.commit()\n",
                "        return True\n",
                "    except sqlite3.IntegrityError:\n",
                "        return False\n",
                "    finally:\n",
                "        conn.close()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "scraping_logic",
            "metadata": {},
            "outputs": [],
            "source": [
                "inserted_count = 0\n",
                "page = 1\n",
                "max_failures = 3\n",
                "failures = 0\n",
                "\n",
                "# Set para detectar duplicatas e parar loop infinito\n",
                "seen_urls = set()\n",
                "\n",
                "while True:\n",
                "    # URL: A primeira p√°gina √© diferente ou igual?\n",
                "    # O site pode ignorar /page/X/ e retornar sempre a home.\n",
                "    if page == 1:\n",
                "        url = \"https://dataprivacy.com.br/category/noticias/\"\n",
                "    else:\n",
                "        url = BASE_URL_TEMPLATE.format(page)\n",
                "        \n",
                "    print(f\"Coletando p√°gina {page}: {url}\")\n",
                "    \n",
                "    try:\n",
                "        r = requests.get(url, headers=HEADERS, timeout=15)\n",
                "        \n",
                "        if r.status_code == 404:\n",
                "            print(\"  P√°gina n√£o encontrada (404). Fim da raspagem.\")\n",
                "            break\n",
                "        \n",
                "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
                "        \n",
                "        articles = soup.find_all(\"article\")\n",
                "        if not articles:\n",
                "             articles = soup.select(\".post, .entry, .card\")\n",
                "        \n",
                "        print(f\"  {len(articles)} not√≠cias encontradas\")\n",
                "        \n",
                "        if len(articles) == 0:\n",
                "            print(\"  Nenhum artigo encontrado.\")\n",
                "            break\n",
                "            \n",
                "        # Verifica duplicidade na p√°gina inteira\n",
                "        # Se todos os artigos desta p√°gina j√° foram vistos, estamos em loop\n",
                "        page_new_urls = 0\n",
                "\n",
                "        for article in articles:\n",
                "            # T√≠tulo\n",
                "            title_tag = article.select_one(\"h2.entry-title, h3.entry-title, h2 a, h3 a\")\n",
                "            if not title_tag:\n",
                "                 title_tag = article.find(\"h2\") or article.find(\"h3\")\n",
                "            \n",
                "            if not title_tag:\n",
                "                continue\n",
                "                \n",
                "            title = title_tag.get_text(strip=True)\n",
                "            \n",
                "            # Link\n",
                "            link_tag = article.find(\"a\")\n",
                "            if title_tag.name == 'a':\n",
                "                link_tag = title_tag\n",
                "            \n",
                "            link = link_tag['href'] if link_tag else \"\"\n",
                "            \n",
                "            # Checa duplicata na sess√£o atual\n",
                "            if link in seen_urls:\n",
                "                continue\n",
                "            \n",
                "            seen_urls.add(link)\n",
                "            page_new_urls += 1\n",
                "            \n",
                "            # Data\n",
                "            date_tag = article.select_one(\"time, .date, .post-date, .entry-date, .published\")\n",
                "            date = \"\"\n",
                "            if date_tag:\n",
                "                date = date_tag.get_text(strip=True)\n",
                "            \n",
                "            if not date:\n",
                "                text = article.get_text()\n",
                "                match = re.search(r'\\d{2}[./]\\d{2}[./]\\d{4}', text)\n",
                "                if match:\n",
                "                    date = match.group(0)\n",
                "            \n",
                "            if insert_article(title, date, link):\n",
                "                inserted_count += 1\n",
                "        \n",
                "        # Se n√£o achou NENHUM URL novo nesta p√°gina, aborta\n",
                "        if page_new_urls == 0:\n",
                "            print(\"‚ö†Ô∏è Todos os artigos desta p√°gina j√° foram lidos. Loop detectado ou fim da pagina√ß√£o real.\")\n",
                "            break\n",
                "        \n",
                "        page += 1\n",
                "        time.sleep(1)\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"Erro na p√°gina {page}: {e}\")\n",
                "        break\n",
                "\n",
                "print(f\"\\nüì• Total de novas not√≠cias inseridas: {inserted_count}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "analysis_load",
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_articles():\n",
                "    conn = sqlite3.connect(DATABASE_NAME)\n",
                "    df = pd.read_sql(\"SELECT * FROM articles\", conn)\n",
                "    conn.close()\n",
                "    return df\n",
                "\n",
                "df_db = load_articles()\n",
                "print(f\"üì¶ Total no banco: {len(df_db)} registros\")\n",
                "display(df_db.head())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "viz",
            "metadata": {},
            "outputs": [],
            "source": [
                "if not df_db.empty:\n",
                "    # 1. Timeline (simples contagem por data string)\n",
                "    date_count = df_db['date'].value_counts().reset_index()\n",
                "    date_count.columns = ['Date', 'Count']\n",
                "    date_count = date_count.sort_values('Date')\n",
                "    \n",
                "    fig1 = px.line(date_count, x='Date', y='Count', title='Frequ√™ncia de Not√≠cias por Data (String)')\n",
                "    fig1.show()\n",
                "    \n",
                "    # 2. Nuvem de Palavras (Treemap)\n",
                "    text = ' '.join(df_db['title'].astype(str)).lower()\n",
                "    words = re.findall(r'\\b\\w{4,}\\b', text)\n",
                "    \n",
                "    stopwords = {'para', 'sobre', 'como', 'pela', 'pelo', 'est√°', 'ser√°', 'entre', 'nesta'}\n",
                "    words = [w for w in words if w not in stopwords]\n",
                "    \n",
                "    wc = pd.Series(words).value_counts().head(30).reset_index()\n",
                "    wc.columns = ['Word', 'Frequency']\n",
                "    \n",
                "    fig2 = px.treemap(wc, path=['Word'], values='Frequency', title='Palavras mais frequentes nos t√≠tulos')\n",
                "    fig2.show()\n",
                "else:\n",
                "    print(\"Sem dados para visualizar.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}